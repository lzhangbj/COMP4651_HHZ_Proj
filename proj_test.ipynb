{"cells":[{"cell_type":"markdown","source":["### 1. Data Processing\n#### 1.1 Data Load"],"metadata":{}},{"cell_type":"code","source":["train_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/train.csv')\nbuilding_metadata_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/building_metadata.csv')\nweather_train_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/weather_train.csv')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["#### 1.2 Data Processing"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import when, avg, col, round\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["### weather train dataset processing ###\n\nw = Window().partitionBy('site_id')\n\n#Replace negative values of 'qty' with Null, as we don't want to consider them while averaging.\n#weather_train_df = weather_train_df.withColumn('qty',when(col('qty')<0,None).otherwise(col('qty')))\nweather_train_df = weather_train_df.withColumn('cloud_coverage',when(col('cloud_coverage').isNull(),avg(col('cloud_coverage')).over(w)).otherwise(col('cloud_coverage')))\n\nweather_train_df = weather_train_df.withColumn(\"cloud_coverage\", round(weather_train_df[\"cloud_coverage\"]).cast('integer'))\n\nweather_train_df = weather_train_df.withColumn('sea_level_pressure',when(col('sea_level_pressure').isNull(),avg(col('sea_level_pressure')).over(w)).otherwise(col('sea_level_pressure')))\n\nweather_train_df = weather_train_df.withColumn('precip_depth_1_hr',when(col('precip_depth_1_hr').isNull(),avg(col('precip_depth_1_hr')).over(w)).otherwise(col('precip_depth_1_hr')))\nweather_train_df = weather_train_df.withColumn(\"precip_depth_1_hr\", round(weather_train_df[\"precip_depth_1_hr\"]).cast('integer'))\n\nweather_train_df=weather_train_df.na.drop()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["### building meta data processing ###\n\nw = Window().partitionBy('primary_use')\nbuilding_metadata_df = building_metadata_df.withColumn('year_built',when(col('year_built').isNull(),avg(col('year_built')).over(w)).otherwise(col('year_built')))\nbuilding_metadata_df = building_metadata_df.withColumn(\"year_built\", round(building_metadata_df[\"year_built\"]).cast('integer'))\n\nbuilding_metadata_df = building_metadata_df.withColumn('floor_count',when(col('floor_count').isNull(),avg(col('floor_count')).over(w)).otherwise(col('floor_count')))\nbuilding_metadata_df = building_metadata_df.withColumn(\"floor_count\", round(building_metadata_df[\"floor_count\"]).cast('integer'))\n\nbuilding_metadata_df=building_metadata_df.na.drop()\n\nbuilding_metadata_df = building_metadata_df.withColumn(\"primary_use\", when(col(\"primary_use\")==\"Education\", 1)\n                                   .when(col(\"primary_use\")==\"Office\", 2)\n                                   .when(col(\"primary_use\")==\"Entertainment/public assembly\", 3)\n                                   .when(col(\"primary_use\")==\"Public services\", 4)\n                                   .when(col(\"primary_use\")==\"Lodging/residential\", 5)\n                                   .when(col(\"primary_use\")==\"Other\", 6)\n                                   .when(col(\"primary_use\")==\"Healthcare\", 7)\n                                   .when(col(\"primary_use\")==\"Parking\", 8)\n                                   .when(col(\"primary_use\")==\"Warehouse/storage\", 9)\n                                   .when(col(\"primary_use\")==\"Manufacturing/industrial\", 10)\n                                   .when(col(\"primary_use\")==\"Retail\", 11)\n                                   .when(col(\"primary_use\")==\"Services\", 12)\n                                   .when(col(\"primary_use\")==\"Technology/science\", 13)\n                                   .when(col(\"primary_use\")==\"Food sales and service\", 14)\n                                   .when(col(\"primary_use\")==\"Utility\", 15)\n                                   .when(col(\"primary_use\")==\"Religious worship\", 16))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["### train dataset processing ###\ntrain_df = train_df.withColumn(\"meter_reading\", when(col(\"meter\")==0, col(\"meter_reading\")*0.293))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# test_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/test.csv')\n# building_metadata_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/building_metadata.csv')\n# weather_test_df = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/weather_test.csv')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["meta_train_df =  building_metadata_df.join(train_df, (building_metadata_df['building_id'] == train_df['building_id']))\ncond = [weather_train_df.site_id == meta_train_df.site_id, weather_train_df.timestamp == meta_train_df.timestamp]\ntrainDF =  weather_train_df.join(meta_train_df, cond)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# meta_test_df =  building_metadata_df.join(test_df, (building_metadata_df['building_id'] == test_df['building_id']))\n# cond = [weather_test_df.site_id == meta_test_df.site_id, weather_test_df.timestamp == meta_test_df.timestamp]\n# testDF =  weather_test_df.join(meta_test_df, cond)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["datasetDF = trainDF.drop(\"timestamp\", \"site_id\", \"building_id\")\ndatasetDF = datasetDF.na.fill(0)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["# datasetTestDF = testDF.drop(\"timestamp\", \"site_id\", \"building_id\")\n# datasetTestDF = datasetTestDF.na.fill(0)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["split15DF, split85DF = datasetDF.randomSplit([15., 85.], seed=190)\n\n# Let's cache these datasets for performance\ntestSetDF = split15DF.cache()\ntrainingSetDF = split85DF.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["### 2. ML and Evaluation Set Up"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.sql.functions import log\nfrom pyspark.ml.evaluation import Evaluator, RegressionEvaluator\nfrom math import sqrt\nfrom statistics import mean\n# cross validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# ***** vectorizer MODEL ****\nfrom pyspark.ml.feature import VectorAssembler\n\nvectorizer = VectorAssembler()\nvectorizer.setInputCols([\"air_temperature\", \"cloud_coverage\", \"dew_temperature\", \"precip_depth_1_hr\", \"sea_level_pressure\", \n                         \"wind_direction\", \"wind_speed\", \"square_feet\", \"year_built\", \"floor_count\", \"meter\", \"primary_use\"])\nvectorizer.setOutputCol(\"features\")\n\nclass RMSLEEvaluator(Evaluator):\n\n    def __init__(self, predictionCol=\"prediction\", labelCol=\"label\"):\n        self.predictionCol = predictionCol\n        self.labelCol = labelCol\n\n    def _evaluate(self, dataset):\n        \"\"\"\n        Returns a random number. \n        Implement here the true metric\n        \"\"\"\n        new_dataset = dataset.withColumn('result_'+self.predictionCol, log(col(self.predictionCol)+1))\n        new_dataset = new_dataset.withColumn('result_'+self.labelCol, log(col(self.labelCol)+1))\n        new_dataset = new_dataset.withColumn('result', (col('result_'+self.predictionCol) - col('result_'+self.labelCol))**2)\n        \n        result = new_dataset.agg(avg(col(\"result\")))\n        result = result.collect()[0][\"avg(result)\"]\n        return sqrt(result)\n      \n    def isLargerBetter(self):\n        return True"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["#### 2.1 Linear Regression\n##### 2.1.1 Linear Regression Pipeline"],"metadata":{}},{"cell_type":"code","source":["# ***** LINEAR REGRESSION MODEL ****\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.regression import LinearRegressionModel\n# Let's initialize our linear regression learner\nlr = LinearRegression()\n\n# Now we set the parameters for the method\nlr.setPredictionCol(\"predicted_meter_reading\")\\\n  .setLabelCol(\"meter_reading\")\\\n  .setMaxIter(100)\\\n  .setRegParam(0.15)\n\n# We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\nlrPipeline = Pipeline()\n\nlrPipeline.setStages([vectorizer, lr])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[81]: Pipeline_2005b69b9767</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["#### 2.1.2 Linear Regression Simple Fitting"],"metadata":{}},{"cell_type":"code","source":["# Let's first train on the entire dataset to see what we get\nlrModel = lrPipeline.fit(trainingSetDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["#### 2.1.3 Linear Regression Simple Evaluation"],"metadata":{}},{"cell_type":"code","source":["# The intercept is as follows:\nintercept = lrModel.stages[1].intercept\n\n# The coefficents (i.e., weights) are as follows:\nweights = lrModel.stages[1].coefficients\n\n# Create a list of the column names (without PE)\nfeaturesNoLabel = [col for col in datasetDF.columns if col != \"meter_reading\" and col != \"primary_use\"]\n\n# Merge the weights and labels\ncoefficents = zip(weights, featuresNoLabel)\n\nequation = \"y = {intercept}\".format(intercept=intercept)\nvariables = []\nfor x in coefficents:\n    weight = abs(x[0])\n    name = x[1]\n    symbol = \"+\" if (x[0] > 0) else \"-\"\n    equation += (\" {} ({} * {})\".format(symbol, weight, name))\n\n# Finally here is our equation\nprint(\"Linear Regression Equation: \" + equation)\n\nresultsDF = lrModel.transform(testSetDF)\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RMSLEEvaluator(predictionCol=\"predicted_meter_reading\", labelCol=\"meter_reading\")\n\n# Run the evaluator on the DataFrame\nrmsle = regEval.evaluate(resultsDF)\n\nprint(\"Simple Root Mean Squared Logarithmtic Error: %.2f\" % rmsle)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Linear Regression Equation: y = -161.49270517166903 - (0.024079192795500377 * air_temperature) + (0.13825604358664045 * cloud_coverage) + (0.08693710452724579 * dew_temperature) - (0.002700260347343527 * precip_depth_1_hr) + (0.04279355860320019 * sea_level_pressure) + (0.0014832816396364843 * wind_direction) + (0.0865212554352526 * wind_speed) + (7.245281257821358e-05 * square_feet) + (0.06123188029759769 * year_built) + (0.8976686419093083 * floor_count) - (7.5292296456378 * meter)\nSimple Root Mean Squared Logarithmtic Error: 1.45\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["#### 2.1.4 Linear Regression Cross Validation Fitting"],"metadata":{}},{"cell_type":"code","source":["# We can reuse the RegressionEvaluator, regEval, to judge the model based on the best Root Mean Squared Error\n# Let's create our CrossValidator with 3 fold cross validation\ncrossval = CrossValidator(estimator=lrPipeline, evaluator=regEval, numFolds=3)\n\n# Let's tune over our regularization parameter from 0.05 to 0.50\nregParam = [x / 200.0 for x in range(1, 11)]\n\n# We'll create a paramter grid using the ParamGridBuilder, and add the grid to the CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, regParam)\n             .build())\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\ncvModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["#### 2.1.5 Linear Regression Cross Validation Evaluation"],"metadata":{}},{"cell_type":"code","source":["# evaluation\nresultsDF = lrModel.transform(testSetDF)\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RMSLEEvaluator(predictionCol=\"predicted_meter_reading\", labelCol=\"meter_reading\")\n\n# Run the evaluator on the DataFrame\nrmsle = regEval.evaluate(resultsDF)\n\nprint(\"Cross Validated Root Mean Squared Logarithmtic Error: %.2f\" % rmsle)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### 2.2 Decision Tree\n#### 2.2.1 Decision Tree Pipeline"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import DecisionTreeRegressor\n\n# Create a DecisionTreeRegressor\ndt = DecisionTreeRegressor()\n\ndt.setPredictionCol(\"predicted_meter_reading\")\\\n  .setLabelCol(\"meter_reading\")\\\n  .setFeaturesCol(\"features\")\\\n  .setMaxBins(100)\n\n# Create a Pipeline\ndtPipeline = Pipeline()\n\n# Set the stages of the Pipeline\ndtPipeline.setStages([vectorizer, dt])"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["#### 2.2.2 Decision Tree Simple Fitting"],"metadata":{}},{"cell_type":"code","source":["# Let's first train on the entire dataset to see what we get\ndtModel = dtPipeline.fit(trainingSetDF)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["#### 2.2.3 Decision Tree Simple Evaluation"],"metadata":{}},{"cell_type":"code","source":["resultsDF = dtModel.transform(testSetDF)\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RMSLEEvaluator(predictionCol=\"predicted_meter_reading\", labelCol=\"meter_reading\")\n\n# Run the evaluator on the DataFrame\nrmsle = regEval.evaluate(resultsDF)\n\nprint(\"Simple Root Mean Squared Logarithmtic Error: %.2f\" % rmsle)\n\n# dtSubmitted = dtModel.transform(datasetTestDF)\n# dtSubmitted = dtSubmitted.select('row_id', 'predicted_meter_reading')\n# dtSubmitted = dtSubmitted.withColumnRenamed('predicted_meter_reading', \"meter_reading\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# file='/FileStore/tables/my.csv'\n# dtSubmitted = dtSubmitted.repartition(1)\n   \n# dtSubmitted.write.format(\"com.databricks.spark.csv\")\\\n#    .option(\"header\", \"true\")\\\n#    .save(file)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["#### 2.2.4 Decision Tree Cross Validation Fitting"],"metadata":{}},{"cell_type":"code","source":["# Let's just reuse our CrossValidator with the new dtPipeline, RegressionEvaluator regEval, and 3 fold cross validation\ncrossval.setEstimator(dtPipeline)\n\n# Let's tune over our dt.maxDepth parameter on the values 2 and 3, create a paramter grid using the ParamGridBuilder\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [3,4,5])\n             .build())\n\n# Add the grid to the CrossValidator\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\ndtModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["#### 2.2.5 Decision Tree Cross Validation Evaluation"],"metadata":{}},{"cell_type":"code","source":["# evaluation\nresultsDF = dtModel.transform(testSetDF)\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RMSLEEvaluator(predictionCol=\"predicted_meter_reading\", labelCol=\"meter_reading\")\n\n# Run the evaluator on the DataFrame\nrmsle = regEval.evaluate(resultsDF)\n\nprint(\"Cross Validated Root Mean Squared Logarithmtic Error: %.2f\" % rmsle)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["### 2.3 Random Forest\n#### 2.3.1 Random Forest Pipeline"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\n\n# Create a DecisionTreeRegressor\nrf = RandomForestRegressor()\n\nrf.setPredictionCol(\"predicted_meter_reading\")\\\n  .setLabelCol(\"meter_reading\")\\\n  .setFeaturesCol(\"features\")\\\n  .setMaxBins(100)\n\n# Create a Pipeline\nrfPipeline = Pipeline()\n\n# Set the stages of the Pipeline\nrfPipeline.setStages([vectorizer, rf])"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["#### 2.3.2 Random Forest Simple Fitting"],"metadata":{}},{"cell_type":"code","source":["# Let's first train on the entire dataset to see what we get\nrfModel = rfPipeline.fit(trainingSetDF)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["#### 2.3.3 Random Forest Simple Evaluation"],"metadata":{}},{"cell_type":"code","source":["resultsDF = rfModel.transform(testSetDF)\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RMSLEEvaluator(predictionCol=\"predicted_meter_reading\", labelCol=\"meter_reading\")\n\n# Run the evaluator on the DataFrame\nrmsle = regEval.evaluate(resultsDF)\n\nprint(\"Simple Root Mean Squared Logarithmtic Error: %.2f\" % rmsle)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["#### 2.3.4 Random Forest Cross Validation Fitting"],"metadata":{}},{"cell_type":"code","source":["# Let's just reuse our CrossValidator with the new dtPipeline, RegressionEvaluator regEval, and 3 fold cross validation\ncrossval.setEstimator(rfPipeline)\n\n# Let's tune over our dt.maxDepth parameter on the values 2 and 3, create a paramter grid using the ParamGridBuilder\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [3,4,5])\n             .build())\n\n# Add the grid to the CrossValidator\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\nrfModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["#### 2.3.5 Random Forest Cross Validation Evaluation"],"metadata":{}},{"cell_type":"code","source":["# evaluation\nresultsDF = rfModel.transform(testSetDF)\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RMSLEEvaluator(predictionCol=\"predicted_meter_reading\", labelCol=\"meter_reading\")\n\n# Run the evaluator on the DataFrame\nrmsle = regEval.evaluate(resultsDF)\n\nprint(\"Cross Validated Root Mean Squared Logarithmtic Error: %.2f\" % rmsle)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["### 2.4 Gradient-Boosted Trees\n#### 2.4.1 Gradient-Boosted Trees Pipeline"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\n\n# Create a DecisionTreeRegressor\ngbt = GBTRegressor()\n\ngbt.setPredictionCol(\"predicted_meter_reading\")\\\n  .setLabelCol(\"meter_reading\")\\\n  .setFeaturesCol(\"features\")\\\n  .setMaxBins(100)\\\n  .setMaxIter(20)\n\n# Create a Pipeline\ngbtPipeline = Pipeline()\n\n# Set the stages of the Pipeline\ngbtPipeline.setStages([vectorizer, gbt])"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#### 2.4.2 Gradient-Boosted Trees Simple Fitting"],"metadata":{}},{"cell_type":"code","source":["# Let's first train on the entire dataset to see what we get\ngbtModel = gbtPipeline.fit(trainingSetDF)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["#### 2.4.3 Gradient-Boosted Trees Simple Evaluation"],"metadata":{}},{"cell_type":"code","source":["resultsDF = gbtModel.transform(testSetDF)\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RMSLEEvaluator(predictionCol=\"predicted_meter_reading\", labelCol=\"meter_reading\")\n\n# Run the evaluator on the DataFrame\nrmsle = regEval.evaluate(resultsDF)\n\nprint(\"Simple Root Mean Squared Logarithmtic Error: %.2f\" % rmsle)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-657403341536024&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>resultsDF <span class=\"ansi-blue-fg\">=</span> gbtModel<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>testSetDF<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Create an RMSE evaluator using the label and predicted columns</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> regEval <span class=\"ansi-blue-fg\">=</span> RMSLEEvaluator<span class=\"ansi-blue-fg\">(</span>predictionCol<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;predicted_meter_reading&#34;</span><span class=\"ansi-blue-fg\">,</span> labelCol<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;meter_reading&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-red-fg\"># Run the evaluator on the DataFrame</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;gbtModel&#39; is not defined</div>"]}}],"execution_count":52},{"cell_type":"markdown","source":["#### 2.4.4 Gradient-Boosted Trees Cross Validation Fitting"],"metadata":{}},{"cell_type":"code","source":["# Let's just reuse our CrossValidator with the new dtPipeline, RegressionEvaluator regEval, and 3 fold cross validation\ncrossval.setEstimator(gbtPipeline)\n\n# Let's tune over our dt.maxDepth parameter on the values 2 and 3, create a paramter grid using the ParamGridBuilder\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [3,4,5])\n             .build())\n\n# Add the grid to the CrossValidator\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\ngbtModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["#### 2.4.5 Gradient-Boosted Trees Cross Validation Evaluation"],"metadata":{}},{"cell_type":"code","source":["# evaluation\nresultsDF = gbtModel.transform(testSetDF)\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RMSLEEvaluator(predictionCol=\"predicted_meter_reading\", labelCol=\"meter_reading\")\n\n# Run the evaluator on the DataFrame\nrmsle = regEval.evaluate(resultsDF)\n\nprint(\"Cross Validated Root Mean Squared Logarithmtic Error: %.2f\" % rmsle)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["### 3. Ensemble Model"],"metadata":{}},{"cell_type":"markdown","source":["#### 3.1 Data Preparation"],"metadata":{}},{"cell_type":"code","source":["# lrModel\n# dtModel\n# rfModel\n# gbtModel\nfrom pyspark.sql.functions import monotonically_increasing_id \n\nlrTrainingSetDF = lrModel.transform(trainingSetDF).withColumnRenamed('predicted_meter_reading', 'lr_predicted').select('lr_predicted', \"meter_reading\")\\\n                          .withColumn('id', monotonically_increasing_id())\ndtTrainingSetDF = dtModel.transform(trainingSetDF).withColumnRenamed('predicted_meter_reading', 'dt_predicted').select('dt_predicted')\\\n                          .withColumn('id', monotonically_increasing_id())\nrfTrainingSetDF = rfModel.transform(trainingSetDF).withColumnRenamed('predicted_meter_reading', 'rf_predicted').select('rf_predicted')\\\n                          .withColumn('id', monotonically_increasing_id())\ngbtTrainingSetDF = gbtModel.transform(trainingSetDF).withColumnRenamed('predicted_meter_reading', 'gbt_predicted').select('gbt_predicted')\\\n                          .withColumn('id', monotonically_increasing_id())\n\nlrTestSetDF = lrModel.transform(testSetDF).withColumnRenamed('predicted_meter_reading', 'lr_predicted').select('lr_predicted')\\\n                          .withColumn('id', monotonically_increasing_id())\ndtTestSetDF = dtModel.transform(testSetDF).withColumnRenamed('predicted_meter_reading', 'dt_predicted').select('dt_predicted')\\\n                          .withColumn('id', monotonically_increasing_id())\nrfTestSetDF = rfModel.transform(testSetDF).withColumnRenamed('predicted_meter_reading', 'rf_predicted').select('rf_predicted')\\\n                          .withColumn('id', monotonically_increasing_id())\ngbtTestSetDF = gbtModel.transform(testSetDF).withColumnRenamed('predicted_meter_reading', 'gbt_predicted').select('gbt_predicted', \"meter_reading\")\\\n                          .withColumn('id', monotonically_increasing_id())"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["from pyspark.sql.types import StructType\n\n# schema = StructType([])\n# leTrainingSetDF = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n# leTrainingSetDF = leTrainingSetDF.join(lrTrainingSetDF, how=\"left_outer\")\nleTrainingSetDF = lrTrainingSetDF.join(dtTrainingSetDF, \"id\")\nleTrainingSetDF = leTrainingSetDF.join(rfTrainingSetDF, \"id\")\nleTrainingSetDF = leTrainingSetDF.join(gbtTrainingSetDF, \"id\")\nleTrainingSetDF = leTrainingSetDF.drop('id')\n\n# leTestSetDF = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n# leTestSetDF = leTestSetDF.join(lrTrainingSetDF, how=\"left_outer\")\nleTestSetDF = lrTestSetDF.join(dtTestSetDF, \"id\")\nleTestSetDF = leTestSetDF.join(rfTestSetDF, \"id\")\nleTestSetDF = leTestSetDF.join(gbtTestSetDF, \"id\")\nleTestSetDF = leTestSetDF.drop('id')\n\nleVectorizer = VectorAssembler()\nleVectorizer.setInputCols([\"lr_predicted\", \"dt_predicted\", \"rf_predicted\", \"gbt_predicted\"])\nleVectorizer.setOutputCol(\"features\")"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["#### 3.2 Linear Ensemble\n##### 3.2.1 Linear Ensemble PipeLine Set Up"],"metadata":{}},{"cell_type":"code","source":["le = LinearRegression(fitIntercept=False)\n\n# Now we set the parameters for the method\nle.setPredictionCol(\"predicted_meter_reading\")\\\n  .setLabelCol(\"meter_reading\")\\\n  .setMaxIter(100)\\\n  .setRegParam(0.15)\n\nlePipeline = Pipeline()\n\nlePipeline.setStages([leVectorizer, le])"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["##### 3.2.2 Linear Ensemble Simple Fitting"],"metadata":{}},{"cell_type":"code","source":["leModel = lePipeline.fit(leTrainingSetDF)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["##### 3.2.3 Linear Ensemble Simple Evaluation"],"metadata":{}},{"cell_type":"code","source":["# The intercept is as follows:\n# intercept = leModel.stages[1].intercept\n\n# The coefficents (i.e., weights) are as follows:\n\nweights = leModel.stages[1].coefficients\n\n# Create a list of the column names (without PE)\nfeaturesNoLabel = [col for col in leTrainingSetDF.columns if col != \"meter_reading\"]\n\n# Merge the weights and labels\ncoefficents = zip(weights, featuresNoLabel)\n\n#equation = \"y = {intercept}\".format(intercept=intercept)\nvariables = []\nfor x in coefficents:\n    weight = abs(x[0])\n    name = x[1]\n    symbol = \"+\" if (x[0] > 0) else \"-\"\n    equation += (\" {} ({} * {})\".format(symbol, weight, name))\n\nprint(\"Linear Regression Equation: \" + equation)\n\nresultsDF = leModel.transform(leTestSetDF)\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RMSLEEvaluator(predictionCol=\"predicted_meter_reading\", labelCol=\"meter_reading\")\n\n# Run the evaluator on the DataFrame\nrmsle = regEval.evaluate(resultsDF)\n\nprint(\"Simple Root Mean Squared Logarithmtic Error: %.2f\" % rmsle)\n"],"metadata":{},"outputs":[],"execution_count":66}],"metadata":{"name":"proj_test","notebookId":3809160049411972},"nbformat":4,"nbformat_minor":0}
